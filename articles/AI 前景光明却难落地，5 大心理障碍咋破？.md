

[AI 前景光明却难落地，5 大心理障碍咋破？](https://mp.weixin.qq.com/s/fRWQjwRyoYR76IyZJzmODQ)

**作者**：哈佛商业评论

---

AI的成功不仅取决于其与日俱增的能力，还取决于人们使用这些能力的意愿。一些人抗拒AI使用的驱动因素是：认为AI太不透明、没有感情、过于僵化和独立，相比而言，与人类互动更可取。了解这些驱动因素至关重要，有助于设计一些干预措施，以提高组织内部与消费者对AI的采用率。







AI创造了一个惊人的悖论：在2023年盖特纳的一项调查中，79%的组织战略者表示，使用AI、自动化以及数据分析技术，对他们未来两年的成功至关重要，但其中只有20%的人表示他们正在日常活动中使用AI。




AI的成功不仅取决于其与日俱增的能力，还取决于人们使用这些能力的意愿。但正如盖特纳调查结果所显示的，AI尚未得到用户青睐。




不幸的是，大多数人对AI如何塑造未来同样持悲观态度。福布斯顾问（Forbes Advisor）平台的研究显示，77%的美国人担心，AI应用会在未来12个月内导致工作机会的流失。80%的人认为，AI增加了他们的私人数据被犯罪分子恶意使用的可能性。还有更悲观的看法：YouGov进行的一项民意调查发现，近一半的美国人认为AI有一天会攻击人类。在对AI如此不信任的情况下，要让员工们心甘情愿地、热切地、全面地使用AI，是一项艰巨的任务。




基于十多年来我对技术应用的研究，包括对约2500名用户深入的定性访谈和实验，我找到了人们抗拒AI使用的驱动因素：人类对AI的基本认知是，AI太不透明、没有感情、过于僵化和独立，相比而言，与人类互动更可取。了解这些驱动因素至关重要，有助于设计一些干预措施，以提高组织内部与消费者对AI的采用率。在本文中，我将详细探讨这些驱动因素，并解释管理者可以如何应对。







人们认为AI太不透明




对用户来说，许多AI工具所依赖的机器学习算法都是难以捉摸的“黑匣子”。这种密不可见的特性挫伤了人们对知识和理解的基本渴望，尤其是当它们给出不确定或出乎意料的结果的时候。研究发现，如果不透明的AI比人类、或简单透明的AI表现更好时，人们会愿意使用它，但如果它的表现与后者大致相当，人们就会止步不前。




人们往往以为人类的决策不像算法那么“黑匣子”，但这种想法其实没有根据。心理学家已经证明，人们几乎无法洞察他人在想什么，因而，我们会使用启发法来理解人类行为。例如，一项研究中，参与者被要求对AI或人类医生在查看皮肤扫描图像后进行癌症诊断的过程做出描述，他们这才意识到，自己对人类诊断过程的了解并不像他们自以为的那样深刻。这一认知让他们不再对使用医疗AI抱有偏见。




对AI工具如何工作做出解释，可以提高它们的接受度，但并非所有的解释都会有效。研究人员发现，相较于简单的解释AI做了什么（例如，它启动了汽车的刹车系统并让汽车停了下来），人们更愿意知道AI工具为什么会做某事（例如，他们更愿意知道自动驾驶汽车刹车是因为前方有障碍物）。




解释的方式至关重要。那些使用“比较推理”（comparative reasoning）的解释，也就是概述为什么不选择某些替代方案的解释，比起不使用比较推理的解释更能提升用户的信任度。例如，一项研究发现，对AI系统为何将某一肿瘤归类为恶性而非良性做出详细描述的解释，比起简单地说“该肿瘤与其他恶性肿瘤相似”这样的解释更可信（即使两种解释可能都是对的）。从根本上说，最令人信服的解释是那些既能阐明所做决定背后的原因，又能阐明为什么会放弃其他选择的解释。




因为表现最佳的AI模型通常比其他模型更复杂、也更难解释，因此，管理者可能需要首先在组织流程中引入相对简单的模型，尤其在员工认同极为重要的情况下。




让解释的复杂度与任务的复杂度之间互相匹配也很重要。研究表明，如果对算法的解释表明该算法对于某一任务来说似乎过于简单，就像AI在诊断癌症时，如果只是将扫描结果与一张肿瘤图片进行了比较，那么用户很可能不会听从AI的指引。但是，如果对算法的解释表明该算法很复杂，比如说AI将扫描结果与成千上万张恶性与良性肿瘤的图片进行了比较，还参考了医学研究成果来支持自己的评估，那么就不会降低用户对AI的依从性。这意味着，在撰写解释之前，你需要了解用户对相应任务的看法。你也应该避免造成你的AI对于该任务过于简单的印象；否则，可能还不如不提供任何解释。







人们认为AI没有情感




虽然消费者倾向于相信AI工具可以具备人类的某些能力，但他们并不认为机器能够体验情感，因此对AI可否完成那些似乎需要情感能力的主观性任务持怀疑态度。对于那些已经能够以人类相同技能水平完成某些主观性任务的AI系统，这种怀疑态度阻碍了人们对它的接受程度。例如，相比人类做出的财务建议，人们对AI提供的财务建议持有同样的开放态度，因为这项任务被视为是客观的。然而，当涉及到诸如约会建议这样的被视为高度主观性的事务时，人们明显更倾向于人类的意见。




企业可以通过以客观用语来表述任务，也就是将重点放在任务可量化且可度量的方面，去解决这一障碍。例如，对于AI生成的约会建议，你可以强调“依据性格评估生成的可量化结果而指导匹配过程”的好处。在线约会服务商OkCupid将其算法与个性评估和海量的用户数据分析相结合；它同时还着重说明其算法是如何针对潜在的匹配对象做出筛选和排序，以找到与用户偏好完美吻合的目标。




企业还可以通过将AI工具拟人化，例如赋予其性别、人类的名字和声音，来提升它们的采用度。在一项使用自动驾驶汽车仿真的研究中，当汽车的AI系统具有人类声音和人类头像等特征时，参与者表现出更加强烈的信任和舒适感。另一个例子是亚马逊的Alexa，它具有女性的性别以及包括名字和声音等在内的一系列类人特征。这些特征形成了一种让用户熟悉的个性，能帮助用户更好地与AI产生共鸣，并能够更加舒适地与其互动。




其他研究人员还发现，那些不愿将AI拟人化的人对AI能力的信任度也较低，从而导致他们抗拒对AI的使用。例如，与和人类电话推销员通话相比，那些不愿将电话推销聊天机器人拟人化的人，往往会更快结束与聊天机器人的通话。




虽然将AI拟人化往往能提升采用率，但有时也会适得其反，就像获取治疗性传播疾病药物这样敏感或尴尬的情况。在这种情况下，消费者往往更喜欢没有人类特征的AI，因为他们相信这样的AI更少评判他人。










人们认为AI太僵化




人们通常认为犯错有助于人类的学习和成长，而不会把犯错当成一个标志，意味着这个缺陷不可更改。但人们经常会认为AI工具是死板的，不善于调整和进化：这种信念很可能源于人们的过往经验，认为机器只是执行有限功能的静态设备。




这种认知会降低人们对技术的信任，并对技术在新场景中能发挥的效能产生担忧。然而，研究表明，当人们得知AI具有自适应学习能力时，消费者对AI的使用率就会上升。即使只是一些暗示AI具有学习潜力的命名，例如将AI称为“机器学习”而不只是 “算法”，也能提高用户的参与度。奈飞会经常宣传它是如何随着收集更多的用户观看习惯数据而不断改进其内容推荐算法的。为了强化这一观点，奈飞将推荐内容打上了“为您”等标签，并会解释说推荐这些内容的原因是 “因为您曾观看了x”，从而进一步增强了用户关于算法的印象，算法是在一直考虑他们不断变化的偏好的。




那些认为AI缺乏灵活性的人们可能会认为，AI会对每个人一视同仁，生搬硬套一刀切的方式方法，因此忽略了个体的独特性。的确，消费者越认为自己与众不同，就越不会使用AI。例如，一项研究表明，参与者越是认为自己的道德特征突出，就会对评估道德品质的AI系统越发抗拒。




与此同时，灵活性和可预测性之间也存在着微妙的平衡。尽管公司强调AI的学习和进化能力，通常会提高AI的采用率，但如果用户觉得系统的输出过于不可预测，这种做法也可能会适得其反。




因为适应性更强的AI系统可以支持更多样的用户互动方式，而其中的一些互动方式可能无法被用于AI训练的数据捕捉到，因此风险也往往更大。而当AI变得更加灵活时，人们以不恰当的方式使用它的可能性也会增加，在这种情况下，算法可能会做出不恰当的反应，给用户和公司带来新的风险。




因此，AI系统必须在灵活性与可预测性和安全性之间取得平衡。它们可以通过纳入用户反馈，并添加可以正确处理意外输入的安全保护措施来做到这一点。







人们认为AI过于自主




无需人类主动干预就能执行任务的AI工具往往会让人们感觉受到了威胁。人类从很早就开始努力掌控自己周围的环境，去实现自己的目标。因此，如果某些创新削弱了他们对局势的掌控力，人类本能地不愿意采用。




AI赋予算法高度的独立性，使其能够制定策略、采取行动并不断完善自身能力，而所有这些应对新情况的调整，都无需人类直接的引导。AI工具不仅仅是提供协助，而是会完全接管某些之前由人类来处理的任务，这一可能性激起了人们强烈的担忧和顾虑。例如，绝大多数美国人（76%）对乘坐自动驾驶汽车感到担忧。同样，人们担心智能家居设备可能会侵犯他们的隐私，偷偷收集他们的个人数据，并会以不可预见的方式使用这些数据。




人们不愿意把任务交给AI的另一个原因，是因为他们认为自己的个人表现要优于AI技术的表现。有意思的是，在一个实验中，我发现，从美国各地选出的1600多名年龄从18岁到86岁的参与者，相比为自己选择的汽车的自动化水平，参与者会为他人选择自动化水平更高的汽车。原因何在？因为他们认为相比自动驾驶汽车，自己是更好的司机，而其他人则不是更好的司机。




为了提高AI系统的利用率，公司可以通过让人们为系统提供输入，也就是形成所谓的 “人类参与式系统”，来找回消费者的掌控感。Nest是一款允许用户对其进行个性化设置的智能家居产品，比如通过手动来调节温控器或去设置特定的时间表。用户可以选择自动学习或手动输入的方式。掌控感还可以通过对产品设计元素的调整来增强。例如，iRobot公司选择将Roomba吸尘器设置为按照可预测的路径移动，而不是按照可能会让吸尘器显得 “很活跃”的不可预测的路径移动。




然而，让人们对AI系统拥有过多的控制权有可能会降低其产出的质量和效率。幸运的是，研究发现，只需保留少量的干预能力就能让消费者感到舒适。因此，营销人员可以对AI系统进行校准，取得人类控制感和系统准确性之间的最佳平衡。







人们更愿意与人类互动




在一项研究中，我考察了人们是更愿意接受人类销售员的服务，还是接受假想的，被描述为在外观和身心能力上与人类无异的AI机器人的服务。在一系列包括与人类或机器人销售员互动的预期舒适度、到访他们所在商店的意愿，以及预期的客户服务水平等衡量标准上，参与者都一致倾向于人类销售员。这一结果源于人们对机器人没有人类的意识，缺乏对意义的理解能力的信念。此外，参与者认为机器人与人类的差异越大（通过参与者对诸如“在道德层面，机器人永远不如人类”等说法的认可程度的打分来衡量），他们表现出的这种偏好就越强烈。




文化背景很可能是反AI倾向中的一个重要因素。例如，比起其他国家，即使是无生命的物体也有灵魂或精神这一信念在日本更被广为接受，这可能会导致与人类高度相似的AI在日本会更容易被接受。




无论公司在AI上投入多少资金，领导团队都必须考虑那些影响人们使用AI的心理障碍。同时也必须认识到，对于我所描述的五种障碍中的任何一个，一些旨在提高接受度的干预措施，都有可能在无意中增加了对AI的抵触。




相较于直接进入解决方案模式，领导者们更应该谨慎行事。每一个AI系统、应用、测试和全面部署，都会遇到不同的障碍。领导者的职责就是认识到这些障碍，帮助客户和员工克服它们。




关键词：AI




朱利安·德弗雷塔斯（Julian De Freitas）| 文 

朱利安·德弗雷塔斯是哈佛商学院市场营销学助理教授。 

DeepL | 译   张炬 | 校   程明霞 | 编辑









推荐阅读















往期推荐






靠低成本领先的企业，真正的杀手锏是什么？

文化差异不是借口，跨国谈判靠的是四大法则

AI当道，人类的护城河在哪儿？




《哈佛商业评论》中文版 联系方式

投稿、广告、内容和商务合作

newmedia@hbrchina.org

↓点击阅读原文进入哈评中文网，阅读更多精品内容